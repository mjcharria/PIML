{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This GitHub serves as a guide for replication and further development of LNNs, consequently it´s heavily commented and explained (for detailed explanation go to LNN for 1M and 1S).\n",
        "# This code stems from various sources, along the code it will be commented in detail and the sources cited from which the code stems\n",
        "# I was heavily influenced by Cranmer´s form of implementation of LNNs (https://github.com/MilesCranmer/lagrangian_nns) however there are\n",
        "# not many simple LNNs available from which to learn, therefore AI was used as a guide to know the overall steps the code should follow (I wrote the code) and for\n",
        "# debugging and optimization of the performace of the code.\n",
        "\n",
        "\n",
        "# Inverse Problem LNN N Masses, N+1 Springs\n",
        "\n",
        "k_vals = [4.1,2,3,4.1,2]\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jacobian, hessian, vmap, random\n",
        "import optax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.integrate import solve_ivp\n",
        "\n",
        "m = 1.0\n",
        "\n",
        "\n",
        "# Generate Data with equations of motion\n",
        "\n",
        "def simulate_system_N(y0, k_vals, t_max=10.0, num_points=100):\n",
        "\n",
        "    N = len(y0) // 2\n",
        "    k_vals = jnp.array(k_vals)\n",
        "\n",
        "    def sum_of_acc(t,y):\n",
        "\n",
        "      x = y[:N]\n",
        "      a = jnp.zeros_like(x)\n",
        "      v = y[N:]\n",
        "\n",
        "      sum = 0\n",
        "      a = a.at[0].set((-k_vals[0] * x[0] + k_vals[1] * (x[1] - x[0]))/m)\n",
        "      a = a.at[-1].set((-k_vals[-1] * x[-1] + k_vals[-2] * (x[-2] - x[-1]))/m)\n",
        "      if N > 2:\n",
        "        for i in range(1,N-1):\n",
        "          a = a.at[i].set((k_vals[i+1] * (x[i+1] - x[i]) + k_vals[i] * (x[i-1] - x[i]))/m)\n",
        "\n",
        "\n",
        "      return jnp.concatenate([v, a])\n",
        "\n",
        "\n",
        "\n",
        "    t_span = (0, t_max)\n",
        "    t_eval = np.linspace(0, t_max, num_points)\n",
        "    sol = solve_ivp(sum_of_acc, t_span, y0, t_eval=t_eval, method='RK45')\n",
        "\n",
        "    t = jnp.array(sol.t)\n",
        "    final = len(y0) // 2\n",
        "    q = jnp.stack([sol.y[i] for i in range(final)], axis=1)\n",
        "    q_dot = jnp.stack([sol.y[i] for i in range(final, len(y0))], axis=1)\n",
        "\n",
        "    dt = t[1] - t[0]\n",
        "    q_ddot = jnp.gradient(q_dot, dt, axis=0)\n",
        "\n",
        "    return t, q, q_dot, q_ddot\n",
        "\n",
        "\n",
        "def data(N =2, n_trajectories=10, num_points=100, k_vals=None):\n",
        "\n",
        "    q_list = []\n",
        "    q_dot_list = []\n",
        "    q_ddot_list = []\n",
        "\n",
        "    rng = np.random.default_rng(0)\n",
        "    if k_vals is None:\n",
        "        k_vals = jnp.ones(N + 1)\n",
        "\n",
        "    for i in range(n_trajectories):\n",
        "\n",
        "        y0 = rng.uniform(low=-2.0, high=2.0, size=(2 * N,))\n",
        "\n",
        "        t_simulated, q, q_dot, q_ddot = simulate_system_N(y0, k_vals, num_points=num_points)\n",
        "\n",
        "        q_list.append(q)\n",
        "        q_dot_list.append(q_dot)\n",
        "        q_ddot_list.append(q_ddot)\n",
        "\n",
        "    return jnp.concatenate(q_list), jnp.concatenate(q_dot_list), jnp.concatenate(q_ddot_list)\n",
        "\n",
        "# Definition LNN\n",
        "class LNN_N_Masses(nn.Module):\n",
        "    log_k: jnp.array\n",
        "    m: float = 1.0\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, q, q_dot):\n",
        "\n",
        "        # As it is a generalization one should acces the length of the last dimensio of q to know how many masses it is describing\n",
        "\n",
        "        x = [q[..., i] for i in range(q.shape[-1])]\n",
        "\n",
        "        v = [q_dot[..., i] for i in range(q_dot.shape[-1])]\n",
        "\n",
        "        log_k = self.param('log_k', lambda _: self.log_k)\n",
        "\n",
        "        k = jnp.exp(log_k)\n",
        "\n",
        "        T = 0.5 * self.m * jnp.sum(jnp.stack([o**2 for o in v]))\n",
        "\n",
        "        V = 0.5*k[0]* x[0]**2 + 0.5*k[-1]* x[-1]**2\n",
        "        for i in range(len(x)-1):\n",
        "            V += 0.5*k[i+1]*(x[i+1] - x[i])**2\n",
        "        return T - V\n",
        "\n",
        "\n",
        "# Definition of Lagrangian calculates lagrangian by LNN and returns acceleration which is a parameter that can be integrated to get q\n",
        "\n",
        "def lagrangian(LNN_returnable, params, q, q_dot):\n",
        "\n",
        "  def Lagrangian_from_LNN( q, q_dot):\n",
        "    #call LNN to return value of Lagrangian\n",
        "    return LNN_returnable.apply(params, q, q_dot).squeeze()\n",
        "\n",
        "  # apply definition\n",
        "  sec = jacobian(grad(Lagrangian_from_LNN, 1), 0)(q, q_dot)\n",
        "  par = grad(Lagrangian_from_LNN, 0)(q, q_dot) - jnp.matmul(sec, q_dot)\n",
        "\n",
        "  H = hessian(Lagrangian_from_LNN, 1)(q, q_dot)\n",
        "\n",
        "  q_ddot_pred = jnp.linalg.pinv(H) @ par\n",
        "  return q_ddot_pred\n",
        "\n",
        "\n",
        "\n",
        "# Loss Function\n",
        "def loss_function(params, model, q, q_dot, q_ddot_data):\n",
        "    # Standard function used in other LNN codes https://docs.jax.dev/en/latest/_autosummary/jax.vmap.html\n",
        "    q_ddot_pred = vmap(lambda q, q_dot: lagrangian(model, params, q, q_dot))(q, q_dot)\n",
        "    # Equivalent to tensorflow´s reduce_mean\n",
        "    return jnp.mean((q_ddot_pred.squeeze() - q_ddot_data)**2)\n",
        "\n",
        "# Standard procedure for jax https://flax.readthedocs.io/en/latest/guides/linen_to_nnx.html#using-trainstate-in-flax-nnx , initialices de model LNN, defines params, model an optimizer\n",
        "def create_train_state(rng, model, learning_rate=1e-3):\n",
        "    init_q = jnp.ones((1,2))\n",
        "    params = model.init(rng, init_q, init_q)\n",
        "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optax.adam(learning_rate))\n",
        "@jax.jit\n",
        "def train(state, q, q_dot, q_ddot):\n",
        "    loss, grads = jax.value_and_grad(loss_function)(state.params, model, q, q_dot, q_ddot)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Training of LNN\n",
        "q, q_dot, q_ddot = data(N = 4, n_trajectories=20, num_points=100, k_vals=k_vals)\n",
        "print(q[1].shape)\n",
        "print(q_dot.shape)\n",
        "print(q_ddot.shape)\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "model = LNN_N_Masses(log_k=jnp.ones(len(q[1])+1))\n",
        "state = create_train_state(rng, model)\n",
        "\n",
        "losses = []\n",
        "for epoch in range(5000):\n",
        "    state, loss = train(state, q, q_dot, q_ddot)\n",
        "    losses.append(loss)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        k = jnp.exp(state.params['params']['log_k'])\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.5f}, k=\", k)\n",
        "\n",
        "# Obtain acceleration from Lagrangian NN\n",
        "q_ddot_pred = vmap(lambda q, q_dot: lagrangian(model, state.params, q, q_dot))(q, q_dot)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prsBwKUWm2nr",
        "outputId": "95e39e01-1150-49de-a5d5-f471e80d7652"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4,)\n",
            "(2000, 4)\n",
            "(2000, 4)\n",
            "Epoch 0, Loss: 1.67663, k= [2.7210016 2.715565  2.7210016 2.7210016 2.7210016]\n",
            "Epoch 100, Loss: 0.93849, k= [3.0066671 2.476532  2.957669  2.992978  2.844326 ]\n",
            "Epoch 200, Loss: 0.50104, k= [3.302761  2.3029873 3.043863  3.2530656 2.6614475]\n",
            "Epoch 300, Loss: 0.23469, k= [3.5734808 2.1807523 3.032013  3.4914532 2.4705684]\n",
            "Epoch 400, Loss: 0.09541, k= [3.7832844 2.0960398 3.003677  3.689067  2.3056965]\n",
            "Epoch 500, Loss: 0.03703, k= [3.9185581 2.0392075 2.9799652 3.8323026 2.1772125]\n",
            "Epoch 600, Loss: 0.01761, k= [3.9930604 2.0040512 2.964362  3.9222112 2.0892043]\n",
            "Epoch 700, Loss: 0.01243, k= [4.0300145 1.9845033 2.9557164 3.9715254 2.0369146]\n",
            "Epoch 800, Loss: 0.01131, k= [4.0471106 1.9747431 2.9515479 3.9955332 2.009967 ]\n",
            "Epoch 900, Loss: 0.01111, k= [4.0545254 1.9703366 2.9497645 4.006018  1.9978083]\n",
            "Epoch 1000, Loss: 0.01108, k= [4.0575056 1.9685268 2.9490848 4.010141  1.9929516]\n",
            "Epoch 1100, Loss: 0.01108, k= [4.05861   1.9678478 2.9488556 4.011599  1.9912231]\n",
            "Epoch 1200, Loss: 0.01108, k= [4.0589848 1.9676145 2.94879   4.0120606 1.9906734]\n",
            "Epoch 1300, Loss: 0.01108, k= [4.059101  1.967541  2.9487739 4.01219   1.9905185]\n",
            "Epoch 1400, Loss: 0.01108, k= [4.0591326 1.9675195 2.948771  4.012217  1.9904814]\n",
            "Epoch 1500, Loss: 0.01108, k= [4.0591354 1.9675165 2.948771  4.012217  1.9904813]\n",
            "Epoch 1600, Loss: 0.01108, k= [4.059136  1.9675165 2.948771  4.012218  1.9904813]\n",
            "Epoch 1700, Loss: 0.01108, k= [4.0591364 1.9675165 2.948771  4.0122185 1.9904813]\n",
            "Epoch 1800, Loss: 0.01108, k= [4.059137  1.9675163 2.948771  4.0122194 1.9904805]\n",
            "Epoch 1900, Loss: 0.01108, k= [4.0591373 1.9675161 2.948771  4.01222   1.9904798]\n",
            "Epoch 2000, Loss: 0.01108, k= [4.059138  1.9675158 2.948771  4.0122204 1.9904794]\n",
            "Epoch 2100, Loss: 0.01108, k= [4.059139  1.9675156 2.948771  4.012221  1.990479 ]\n",
            "Epoch 2200, Loss: 0.01108, k= [4.0591393 1.9675152 2.948771  4.012222  1.9904785]\n",
            "Epoch 2300, Loss: 0.01108, k= [4.0591397 1.967515  2.948771  4.0122223 1.9904778]\n",
            "Epoch 2400, Loss: 0.01108, k= [4.05914   1.9675149 2.948771  4.012223  1.9904773]\n",
            "Epoch 2500, Loss: 0.01108, k= [4.05914   1.9675148 2.948771  4.0122232 1.9904773]\n",
            "Epoch 2600, Loss: 0.01108, k= [4.0591407 1.9675145 2.948771  4.0122232 1.9904768]\n",
            "Epoch 2700, Loss: 0.01108, k= [4.059141  1.9675144 2.9487705 4.012224  1.9904765]\n",
            "Epoch 2800, Loss: 0.01108, k= [4.0591416 1.9675143 2.9487705 4.012224  1.990476 ]\n",
            "Epoch 2900, Loss: 0.01108, k= [4.0591416 1.9675142 2.9487705 4.0122247 1.9904757]\n",
            "Epoch 3000, Loss: 0.01108, k= [4.059142  1.967514  2.9487703 4.012225  1.9904753]\n",
            "Epoch 3100, Loss: 0.01108, k= [4.059142  1.967514  2.9487703 4.012225  1.9904753]\n",
            "Epoch 3200, Loss: 0.01108, k= [4.0591426 1.9675138 2.9487703 4.0122256 1.9904747]\n",
            "Epoch 3300, Loss: 0.01108, k= [4.0591426 1.9675138 2.94877   4.012226  1.9904745]\n",
            "Epoch 3400, Loss: 0.01108, k= [4.059143  1.9675137 2.94877   4.012226  1.9904745]\n",
            "Epoch 3500, Loss: 0.01108, k= [4.059143  1.9675137 2.9487696 4.0122266 1.990474 ]\n",
            "Epoch 3600, Loss: 0.01108, k= [4.0591435 1.9675136 2.9487696 4.012227  1.9904736]\n",
            "Epoch 3700, Loss: 0.01108, k= [4.0591435 1.9675136 2.9487696 4.012227  1.9904736]\n",
            "Epoch 3800, Loss: 0.01108, k= [4.059144  1.9675133 2.9487696 4.0122275 1.9904732]\n",
            "Epoch 3900, Loss: 0.01108, k= [4.059144  1.9675133 2.9487696 4.0122275 1.9904732]\n",
            "Epoch 4000, Loss: 0.01108, k= [4.059144  1.9675133 2.9487696 4.0122275 1.9904732]\n",
            "Epoch 4100, Loss: 0.01108, k= [4.0591445 1.9675132 2.948769  4.012228  1.9904728]\n",
            "Epoch 4200, Loss: 0.01108, k= [4.0591445 1.9675132 2.948769  4.012228  1.9904728]\n",
            "Epoch 4300, Loss: 0.01108, k= [4.0591445 1.9675132 2.948769  4.0122285 1.9904723]\n",
            "Epoch 4400, Loss: 0.01108, k= [4.0591445 1.9675131 2.948769  4.0122285 1.9904723]\n",
            "Epoch 4500, Loss: 0.01108, k= [4.059145  1.967513  2.948769  4.0122285 1.9904723]\n",
            "Epoch 4600, Loss: 0.01108, k= [4.059145 1.967513 2.948769 4.012229 1.990472]\n",
            "Epoch 4700, Loss: 0.01108, k= [4.059145 1.967513 2.948769 4.012229 1.990472]\n",
            "Epoch 4800, Loss: 0.01108, k= [4.059145 1.967513 2.948769 4.012229 1.990472]\n",
            "Epoch 4900, Loss: 0.01108, k= [4.0591455 1.9675128 2.948769  4.012229  1.990472 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lnn 4 masas\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jacobian, hessian, vmap, random\n",
        "import optax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "m = 1.0\n",
        "true_ks = jnp.array([1,1,1,1,2,2])\n",
        "\n",
        "springs = [(0, 1), (2, 3), (0, 2), (1, 3), (0, 3), (1, 2)]\n",
        "\n",
        "x0 = jnp.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]])\n",
        "a_side = jnp.array([jnp.linalg.norm(x0[a] - x0[b]) for a, b in springs])\n",
        "\n",
        "\n",
        "# Generate data from equations, on idea similar to N masses but generalized to 2D (calculation of direction of mass effect on others)\n",
        "def simulate_system_2D(T=10.0, dt=0.01, key=random.PRNGKey(0)):\n",
        "\n",
        "    steps = int(T/dt)\n",
        "    key1, key2 = random.split(key)\n",
        "\n",
        "    def cal_of_acc(x1, x2, k, a_fijo):\n",
        "      # Definition contribution mass j in i\n",
        "      abs = x2 - x1\n",
        "      norm = jnp.linalg.norm(abs)\n",
        "\n",
        "      direction = abs/norm\n",
        "      return k * (norm - a_fijo) * direction\n",
        "\n",
        "\n",
        "    xs = x0 + 0.05 * random.normal(key1, (4, 2))\n",
        "    vs = 0.05 * random.normal(key2, (4, 2))\n",
        "\n",
        "    # calculate rest a = lattice parameter (side of square)\n",
        "    a_side = jnp.array([jnp.linalg.norm(x0[a] - x0[b]) for a, b in springs])\n",
        "\n",
        "    pos_total = []\n",
        "    vel_total = []\n",
        "    acc_total = []\n",
        "\n",
        "    for i in range(steps):\n",
        "\n",
        "        sum_acc = jnp.zeros((4, 2))\n",
        "\n",
        "        for i, (a, b) in enumerate(springs):\n",
        "            F = cal_of_acc(xs[a], xs[b], true_ks[i], a_side[i])/m\n",
        "\n",
        "            sum_acc = sum_acc.at[a].add(F)\n",
        "            sum_acc = sum_acc.at[b].add(-F)\n",
        "\n",
        "        acc = sum_acc\n",
        "\n",
        "        #definition update velocity and trajectory\n",
        "        vs = vs + acc* dt\n",
        "        xs = xs + vs *dt\n",
        "\n",
        "        pos_total.append(xs)\n",
        "        vel_total.append(vs)\n",
        "        acc_total.append(acc)\n",
        "\n",
        "    return jnp.stack(pos_total), jnp.stack(vel_total), jnp.stack(acc_total)\n",
        "\n",
        "\n",
        "\n",
        "# LNN definition, same as N Mass and N+1 Springs but sumatory for other total velocities and lenghts minus the normal length of spring\n",
        "\n",
        "class LNN_2D_Masses(nn.Module):\n",
        "    log_ks: jnp.array\n",
        "    a_side: jnp.ndarray\n",
        "    m: float = 1.0\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, q, q_dot):\n",
        "\n",
        "        # As it is a generalization one should acces the length of the last dimensio of q to know how many masses it is describing\n",
        "\n",
        "        x = [q[..., i] for i in range(q.shape[-1])]\n",
        "\n",
        "        v = [q_dot[..., i] for i in range(q_dot.shape[-1])]\n",
        "\n",
        "        log_ks = self.param('log_ks', lambda _: self.log_ks)\n",
        "        print(log_ks)\n",
        "\n",
        "        #ks = [log_ks[i] for i in range(log_ks.shape[-1])]\n",
        "\n",
        "        ks = jnp.exp(log_ks)\n",
        "        print(ks)\n",
        "\n",
        "        T = 0.5 * self.m * jnp.sum(jnp.stack([o**2 for o in v]))\n",
        "        V = 0\n",
        "        for i, (a, b) in enumerate(springs):\n",
        "            V += 0.5*ks[i]* (jnp.linalg.norm(q[a] - q[b]) - a_side[i])** 2\n",
        "        return T - V\n",
        "\n",
        "def lagrangian(LNN_returnable, params, q_new, q_dot_new):\n",
        "\n",
        "  def Lagrangian_from_LNN( q_2D, q_dot_2D):\n",
        "    #call LNN to return value of Lagrangian\n",
        "    q = q_2D.reshape((4, 2))\n",
        "    q_dot = q_dot_2D.reshape((4, 2))\n",
        "    return LNN_returnable.apply(params, q, q_dot).squeeze()\n",
        "\n",
        "  # apply definition\n",
        "  q, q_dot = q_new.reshape(-1), q_dot_new.reshape(-1)\n",
        "  sec = jacobian(grad(Lagrangian_from_LNN, 1), 0)(q, q_dot)\n",
        "  par = grad(Lagrangian_from_LNN, 0)(q, q_dot) - jnp.matmul(sec, q_dot)\n",
        "\n",
        "  H = hessian(Lagrangian_from_LNN, 1)(q, q_dot)\n",
        "\n",
        "  q_ddot_pred = jnp.linalg.pinv(H) @ par\n",
        "  return q_ddot_pred.reshape((4, 2))\n",
        "\n",
        "# Loss Function\n",
        "def loss_function(params, model, q, q_dot, q_ddot_data):\n",
        "    # Standard function used in other LNN codes https://docs.jax.dev/en/latest/_autosummary/jax.vmap.html\n",
        "    q_ddot_pred = vmap(lambda q, q_dot: lagrangian(model, params, q, q_dot))(q, q_dot)\n",
        "    # Equivalent to tensorflow´s reduce_mean\n",
        "    return jnp.mean((q_ddot_pred - q_ddot_data)**2)\n",
        "\n",
        "# Standard procedure for jax https://flax.readthedocs.io/en/latest/guides/linen_to_nnx.html#using-trainstate-in-flax-nnx , initialices de model LNN, defines params, model an optimizer\n",
        "def create_train_state(rng, model, learning_rate=1e-3):\n",
        "    params = model.init(rng, jnp.ones((4,2)), jnp.ones((4,2)))\n",
        "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optax.adam(learning_rate))\n",
        "@jax.jit\n",
        "def train(state, q, q_dot, q_ddot):\n",
        "    loss, grads = jax.value_and_grad(loss_function)(state.params, model, q, q_dot, q_ddot)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Training of model (Standard)\n",
        "q, q_dot, q_ddot = simulate_system_2D()\n",
        "log_ks_init = jnp.zeros(6)\n",
        "model = LNN_2D_Masses(log_ks=log_ks_init, a_side=a_side)\n",
        "state = create_train_state(random.PRNGKey(0), model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "losses = []\n",
        "for epoch in range(3000):\n",
        "    state, loss = train(state, q, q_dot, q_ddot)\n",
        "    losses.append(loss)\n",
        "    if epoch % 300 == 0:\n",
        "        ks_learned = jnp.exp(state.params['params']['log_ks'])\n",
        "        print(f\"Epoch {epoch}, Loss={loss:.6f}, ks={ks_learned}\")\n",
        "\n",
        "# Graph\n",
        "ks_learned = jnp.exp(state.params['params']['log_ks'])\n",
        "\n",
        "\n",
        "print(\"Predictions\", ks_learned)\n",
        "print(\"Real k:\", true_ks)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxjyJaji9NRb",
        "outputId": "6786abff-dcdd-426b-e173-2fae5cdd4d9d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0.]\n",
            "[1. 1. 1. 1. 1. 1.]\n",
            "Traced<ShapedArray(float32[6])>with<JVPTrace> with\n",
            "  primal = Traced<ShapedArray(float32[6])>with<DynamicJaxprTrace>\n",
            "  tangent = Traced<ShapedArray(float32[6])>with<JaxprTrace> with\n",
            "    pval = (ShapedArray(float32[6]), None)\n",
            "    recipe = LambdaBinding()\n",
            "Traced<ShapedArray(float32[6])>with<JVPTrace> with\n",
            "  primal = Traced<ShapedArray(float32[6])>with<DynamicJaxprTrace>\n",
            "  tangent = Traced<ShapedArray(float32[6])>with<JaxprTrace> with\n",
            "    pval = (ShapedArray(float32[6]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x798940c7d5e0>, in_tracers=(Traced<ShapedArray(float32[6]):JaxprTrace>, Traced<ShapedArray(float32[6]):JaxprTrace>), out_tracer_refs=[<weakref at 0x798940bcf650; to 'JaxprTracer' at 0x798940bcd080>], out_avals=[ShapedArray(float32[6])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[6] b:f32[6]. let c:f32[6] = mul a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'exp', 'keep_unused': False, 'inline': True, 'compiler_options_kvs': ()}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x798948d3f100>, ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=True, cur_abstract_mesh=AbstractMesh((), axis_types={}), xla_metadata=None))\n",
            "Traced<ShapedArray(float32[6])>with<JVPTrace> with\n",
            "  primal = Traced<ShapedArray(float32[6])>with<DynamicJaxprTrace>\n",
            "  tangent = Traced<ShapedArray(float32[6])>with<JaxprTrace> with\n",
            "    pval = (ShapedArray(float32[6]), None)\n",
            "    recipe = LambdaBinding()\n",
            "Traced<ShapedArray(float32[6])>with<JVPTrace> with\n",
            "  primal = Traced<ShapedArray(float32[6])>with<DynamicJaxprTrace>\n",
            "  tangent = Traced<ShapedArray(float32[6])>with<JaxprTrace> with\n",
            "    pval = (ShapedArray(float32[6]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x798940c7cd60>, in_tracers=(Traced<ShapedArray(float32[6]):JaxprTrace>, Traced<ShapedArray(float32[6]):JaxprTrace>), out_tracer_refs=[<weakref at 0x798940a78e00; to 'JaxprTracer' at 0x798940a7b290>], out_avals=[ShapedArray(float32[6])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[6] b:f32[6]. let c:f32[6] = mul a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'exp', 'keep_unused': False, 'inline': True, 'compiler_options_kvs': ()}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x798940a1f250>, ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=True, cur_abstract_mesh=AbstractMesh((), axis_types={}), xla_metadata=None))\n",
            "Traced<ShapedArray(float32[6])>with<JVPTrace> with\n",
            "  primal = Traced<ShapedArray(float32[6])>with<DynamicJaxprTrace>\n",
            "  tangent = Traced<ShapedArray(float32[6])>with<JaxprTrace> with\n",
            "    pval = (ShapedArray(float32[6]), None)\n",
            "    recipe = LambdaBinding()\n",
            "Traced<ShapedArray(float32[6])>with<JVPTrace> with\n",
            "  primal = Traced<ShapedArray(float32[6])>with<DynamicJaxprTrace>\n",
            "  tangent = Traced<ShapedArray(float32[6])>with<JaxprTrace> with\n",
            "    pval = (ShapedArray(float32[6]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x798940dec910>, in_tracers=(Traced<ShapedArray(float32[6]):JaxprTrace>, Traced<ShapedArray(float32[6]):JaxprTrace>), out_tracer_refs=[<weakref at 0x798940a0c1d0; to 'JaxprTracer' at 0x798940a0f880>], out_avals=[ShapedArray(float32[6])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[6] b:f32[6]. let c:f32[6] = mul a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'exp', 'keep_unused': False, 'inline': True, 'compiler_options_kvs': ()}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x798940a2a080>, ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=True, cur_abstract_mesh=AbstractMesh((), axis_types={}), xla_metadata=None))\n",
            "Epoch 0, Loss=0.001001, ks=[1.0010003 1.0010004 1.0010004 1.0010004 1.0010005 1.0010005]\n",
            "Epoch 300, Loss=0.000415, ks=[1.009967  1.12784   1.0867044 1.226871  1.3347646 1.3370305]\n",
            "Epoch 600, Loss=0.000094, ks=[1.0042216 1.0643193 1.0433009 1.1631166 1.6880687 1.6924698]\n",
            "Epoch 900, Loss=0.000010, ks=[1.0001456 1.0190634 1.0143214 1.0543516 1.9158598 1.9226202]\n",
            "Epoch 1200, Loss=0.000004, ks=[0.9988153 1.004365  1.0052637 1.0068105 1.9840897 1.9924867]\n",
            "Epoch 1500, Loss=0.000004, ks=[0.99859846 1.002151   1.0039173  0.9972165  1.9939862  2.0029058 ]\n",
            "Epoch 1800, Loss=0.000004, ks=[0.99857914 1.0019703  1.0038072  0.9963051  1.99478    2.0037751 ]\n",
            "Epoch 2100, Loss=0.000004, ks=[0.99857825 1.0019628  1.0038027  0.9962637  1.9948108  2.0038116 ]\n",
            "Epoch 2400, Loss=0.000004, ks=[0.9985782 1.0019627 1.0038025 0.9962633 1.9948114 2.003812 ]\n",
            "Epoch 2700, Loss=0.000004, ks=[0.9985782 1.0019627 1.0038024 0.9962631 1.9948119 2.0038128]\n",
            "Predictions [0.9985782 1.0019625 1.0038024 0.9962629 1.9948124 2.0038133]\n",
            "Real k: [1 1 1 1 2 2]\n"
          ]
        }
      ]
    }
  ]
}